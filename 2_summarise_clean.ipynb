{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "1. Use GPT to summarise full doc with map-reduce method\n",
    "2. Use Davinci to summarise full doc with map-reduce method\n",
    "3. Use GPT to summarise sections by title\n",
    "4. Try Vector Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_SAVE_DIR = Path(\"data\", \"summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\", \"raw\")\n",
    "toc_2015_fname = Path(DATA_DIR, \"Jan 2015.docx\")\n",
    "toc_2023_fname = Path(DATA_DIR, \"Mar 2023.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_2015 = Docx2txtLoader(str(toc_2015_fname))  # str reqd for loader\n",
    "data_2015 = loader_2015.load()\n",
    "loader_2023 = Docx2txtLoader(str(toc_2023_fname))\n",
    "data_2023 = loader_2023.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole Document - GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4 = ChatOpenAI(model_name=\"gpt-4\")\n",
    "# gpt4 has up to 8,192 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Reduce Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map\n",
    "map_template = \"\"\"\n",
    "Write a concise summary of the following:\n",
    "\"{docs}\"\n",
    "CONCISE SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "Return your response in bullet points which covers the key points of the text.\n",
    "```{doc_summaries}```\n",
    "BULLET POINT SUMMARY:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Reduce Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=gpt4, prompt=map_prompt)\n",
    "\n",
    "# Run chain\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=gpt4, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_2015 = text_splitter.split_documents(data_2015)\n",
    "split_2023 = text_splitter.split_documents(data_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lengths_2015 = [\n",
    "    gpt4.get_num_tokens(split_2015[i].page_content) for i in range(len(split_2015))\n",
    "]\n",
    "\n",
    "token_lengths_2023 = [\n",
    "    gpt4.get_num_tokens(split_2023[i].page_content) for i in range(len(split_2023))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 T&Cs has been split into 5 docs. Made up of [3693, 3626, 3732, 3771, 3738] length tokens.\n",
      "2023 T&Cs has been split into 3 docs.Made up of [3601, 3479, 3199] length tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"2015 T&Cs has been split into {len(split_2015)} docs. Made up of {token_lengths_2015} length tokens.\"\n",
    ")\n",
    "print(\n",
    "    f\"2023 T&Cs has been split into {len(split_2023)} docs.Made up of {token_lengths_2023} length tokens.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-1ivxXVFNax3ENVHReVeOJ0Iz on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-1ivxXVFNax3ENVHReVeOJ0Iz on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-1ivxXVFNax3ENVHReVeOJ0Iz on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-1ivxXVFNax3ENVHReVeOJ0Iz on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-1ivxXVFNax3ENVHReVeOJ0Iz on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-1ivxXVFNax3ENVHReVeOJ0Iz on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    }
   ],
   "source": [
    "output_map_reduce_15 = map_reduce_chain.run(split_2015)\n",
    "output_map_reduce_23 = map_reduce_chain.run(split_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(SUMMARY_SAVE_DIR, \"gpt4_map_reduce_summarized_2015.txt\"), \"w\") as f:\n",
    "    f.write(output_map_reduce_15)\n",
    "with open(Path(SUMMARY_SAVE_DIR, \"gpt4_map_reduce_summarized_2023.txt\"), \"w\") as f:\n",
    "    f.write(output_map_reduce_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 map-reduce summary contains 328 words\n",
      "2023 map-reduce summary contains 280 words\n"
     ]
    }
   ],
   "source": [
    "print(f\"2015 map-reduce summary contains {len(output_map_reduce_15.split())} words\")\n",
    "print(f\"2023 map-reduce summary contains {len(output_map_reduce_23.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole Document - Davinci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "davinci = OpenAI(temperature=0, max_tokens=6000, model=\"davinci-002\")\n",
    "# Davinci allows up to 16,384 tokens\n",
    "# Can also try text-davnci-003 but need to adjust token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=davinci, prompt=map_prompt)\n",
    "\n",
    "# Run chain\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=davinci, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=6000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=6000, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to recreate the chain here, because davinci allows up to 16k tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-1ivxXVFNax3ENVHReVeOJ0Iz on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-1ivxXVFNax3ENVHReVeOJ0Iz on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-1ivxXVFNax3ENVHReVeOJ0Iz on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-1ivxXVFNax3ENVHReVeOJ0Iz on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-1ivxXVFNax3ENVHReVeOJ0Iz on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for gpt-4 in organization org-1ivxXVFNax3ENVHReVeOJ0Iz on tokens per min. Limit: 10000 / min. Please try again in 6ms. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    }
   ],
   "source": [
    "output_map_reduce_davinci_15 = map_reduce_chain.run(split_2015)\n",
    "output_map_reduce_davinci_23 = map_reduce_chain.run(split_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(SUMMARY_SAVE_DIR, \"davinci_map_reduce_summarized_2015.txt\"), \"w\") as f:\n",
    "    f.write(output_map_reduce_davinci_15)\n",
    "with open(Path(SUMMARY_SAVE_DIR, \"davinci_map_reduce_summarized_2023.txt\"), \"w\") as f:\n",
    "    f.write(output_map_reduce_davinci_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 davinci map-reduce summary contains 360 words\n",
      "2023 davinci map-reduce summary contains 261 words\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"2015 davinci map-reduce summary contains {len(output_map_reduce_davinci_15.split())} words\"\n",
    ")\n",
    "print(\n",
    "    f\"2023 davinci map-reduce summary contains {len(output_map_reduce_davinci_23.split())} words\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarising by section\n",
    "\n",
    "Plan here is to manually split the documnets into sections that make sense, and have the LLM summarise each section - in the hopes that more of the relevent key information is maintained.\n",
    "\n",
    "## Splitting docs by sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_2015 = data_2015[0].page_content\n",
    "raw_text_2023 = data_2023[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 T&S contain 15399 words\n",
      "2023 T&S contain 8386 words\n"
     ]
    }
   ],
   "source": [
    "print(f\"2015 T&S contain {len(raw_text_2015.split())} words\")\n",
    "print(f\"2023 T&S contain {len(raw_text_2023.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015 Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiters_2015 = [\n",
    "    \"A. ITUNES STORE, MAC APP STORE, APP STORE AND IBOOKS STORE TERMS OF SALE\",\n",
    "    \"B. ITUNES STORE TERMS AND CONDITIONS\",\n",
    "    \"C. MAC APP STORE, APP STORE AND IBOOKS STORE TERMS AND CONDITIONS\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_2015 = raw_text_2015[\n",
    "    200:\n",
    "]  # removing the initial text as this matches the delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_between_substrings(input_str: str, start: str, end: Optional[str]):\n",
    "    if end is None:\n",
    "        return (input_str.split(start))[1]\n",
    "    return (input_str.split(start))[1].split(end)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_section = string_between_substrings(\n",
    "    raw_text_2015, delimiters_2015[0], delimiters_2015[1]\n",
    ")\n",
    "\n",
    "b_section = string_between_substrings(\n",
    "    raw_text_2015, delimiters_2015[1], delimiters_2015[2]\n",
    ")\n",
    "\n",
    "c_section = string_between_substrings(raw_text_2015, delimiters_2015[2], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Section length: 2280 words,\n",
      "      B Section length: 5186 words,\n",
      "      C Section length: 7767 words \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"\"\"A Section length: {len(a_section.split())} words,\n",
    "      B Section length: {len(b_section.split())} words,\n",
    "      C Section length: {len(c_section.split())} words \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_doc_2015 = text_splitter.create_documents([a_section])\n",
    "b_doc_2015 = text_splitter.create_documents([b_section])\n",
    "c_doc_2015 = text_splitter.create_documents([c_section])\n",
    "\n",
    "a_split_2015 = text_splitter.split_documents(a_doc_2015)\n",
    "b_split_2015 = text_splitter.split_documents(b_doc_2015)\n",
    "c_split_2015 = text_splitter.split_documents(c_doc_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=gpt4, prompt=map_prompt)\n",
    "\n",
    "# Run chain\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=gpt4, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_map_reduce_15_a = map_reduce_chain.run(a_split_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_map_reduce_15_b = map_reduce_chain.run(b_split_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_map_reduce_15_c = map_reduce_chain.run(c_split_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_2015_summary = (\n",
    "    output_map_reduce_15_a\n",
    "    + \"\\n\"\n",
    "    + output_map_reduce_15_b\n",
    "    + \"\\n\"\n",
    "    + output_map_reduce_15_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(SUMMARY_SAVE_DIR, \"2015_sections_summary.txt\"), \"w\") as f:\n",
    "    f.write(sections_2015_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2023 Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiters_2023 = [\n",
    "    \"A. INTRODUCTION TO OUR SERVICES\",\n",
    "    # \"B. USING OUR SERVICES\",\n",
    "    \"C. YOUR SUBMISSIONS TO OUR SERVICES\",\n",
    "    # \"D. FAMILY SHARING\",\n",
    "    # \"E. SERIES PASS AND MULTI-PASS\",\n",
    "    # \"F. ADDITIONAL APP STORE TERMS (EXCLUDING APPLE ARCADE APPS)\",\n",
    "    \"G. ADDITIONAL TERMS FOR CONTENT ACQUIRED FROM THIRD PARTIES\",\n",
    "    # \"H. ADDITIONAL APPLE MUSIC TERMS\",\n",
    "    # \"I. ADDITIONAL APPLE FITNESS+ TERMS\",\n",
    "    # \"J. CARRIER MEMBERSHIP\",\n",
    "    # \"K. MISCELLANEOUS TERMS APPLICABLE TO ALL SERVICES\",\n",
    "]\n",
    "# I've manually split these into lengths of around 2500 - 3000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A. INTRODUCTION TO OUR SERVICES',\n",
       " 'C. YOUR SUBMISSIONS TO OUR SERVICES',\n",
       " 'G. ADDITIONAL TERMS FOR CONTENT ACQUIRED FROM THIRD PARTIES']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delimiters_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_section_23 = string_between_substrings(\n",
    "    raw_text_2023, delimiters_2023[0], delimiters_2023[1]\n",
    ")\n",
    "\n",
    "b_section_23 = string_between_substrings(\n",
    "    raw_text_2023, delimiters_2023[1], delimiters_2023[2]\n",
    ")\n",
    "\n",
    "c_section_23 = string_between_substrings(raw_text_2023, delimiters_2023[2], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Section length: 2654 words,\n",
      "      B Section length: 2996 words,\n",
      "      C Section length: 2692 words \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"\"\"A Section length: {len(a_section_23.split())} words,\n",
    "      B Section length: {len(b_section_23.split())} words,\n",
    "      C Section length: {len(c_section_23.split())} words \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_doc_2023 = text_splitter.create_documents([a_section_23])\n",
    "b_doc_2023 = text_splitter.create_documents([b_section_23])\n",
    "c_doc_2023 = text_splitter.create_documents([c_section_23])\n",
    "\n",
    "a_split_2023 = text_splitter.split_documents(a_doc_2023)\n",
    "b_split_2023 = text_splitter.split_documents(b_doc_2023)\n",
    "c_split_2023 = text_splitter.split_documents(c_doc_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_map_reduce_23_a = map_reduce_chain.run(a_split_2023)\n",
    "output_map_reduce_23_b = map_reduce_chain.run(b_split_2023)\n",
    "output_map_reduce_23_c = map_reduce_chain.run(c_split_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_2023_summary = (\n",
    "    output_map_reduce_23_a\n",
    "    + \"\\n\"\n",
    "    + output_map_reduce_23_b\n",
    "    + \"\\n\"\n",
    "    + output_map_reduce_23_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(SUMMARY_SAVE_DIR, \"2023_sections_summary.txt\"), \"w\") as f:\n",
    "    f.write(sections_2023_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
